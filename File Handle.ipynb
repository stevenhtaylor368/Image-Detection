{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images loaded in\n",
      "Computing Features for  train\\car\n",
      "sifting done\n",
      "Computing Features for  train\\face\n",
      "sifting done\n",
      "Computing Features for  train\\leaf\n",
      "sifting done\n",
      "Computing Features for  train\\motorcycle\n",
      "sifting done\n",
      "Computing Features for  train\\plane\n",
      "sifting done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'descriptor_vstack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-195595e1a178>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images loaded in\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;31m#print(imlist[word])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-195595e1a178>\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(imlist, count)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# perform clustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mbov_descriptor_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatND\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescriptor_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbov_descriptor_stack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevelopVocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptor_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescriptor_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-195595e1a178>\u001b[0m in \u001b[0;36mcluster\u001b[1;34m(vStack)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mmega_histogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mclf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mkmeans_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescriptor_vstack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmega_histogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'descriptor_vstack' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "n_clusters = 20\n",
    "clf = SVC()\n",
    "def formatND(l):\n",
    "    descriptor_vstack = None\n",
    "    vStack = np.array(l[0])\n",
    "    for remaining in l[1:]:\n",
    "        vStack = np.vstack((vStack, remaining))\n",
    "    descriptor_vstack = vStack.copy()\n",
    "    return vStack\n",
    "def cluster(vStack):\n",
    "    kmeans_obj = KMeans(n_clusters = n_clusters)\n",
    "    kmeans_ret = None\n",
    "    mega_histogram = None\n",
    "    clf  = SVC()\n",
    "    kmeans_ret = kmeans_obj.fit_predict(descriptor_vstack)\n",
    "    \n",
    "def standardize(mega_histogram, std=None):\n",
    "    \"\"\"\n",
    "    standardize is required to normalize the distribution\n",
    "    wrt sample size and features. If not normalized, the classifier may become\n",
    "    biased due to steep variances.\n",
    "    \"\"\"\n",
    "    if std is None:\n",
    "        scale = StandardScaler().fit(mega_histogram)\n",
    "        mega_histogram = scale.transform(self.mega_histogram)\n",
    "    else:\n",
    "        print (\"STD not none. External STD supplied\")\n",
    "        mega_histogram = std.transform(mega_histogram)\n",
    "    return mega_histogram\n",
    "        \n",
    "def developVocabulary(n_images, descriptor_list, kmeans_ret = None):\n",
    "    mega_histogram = np.array([np.zeros(n_clusters) for i in range(n_images)])\n",
    "    old_count = 0\n",
    "    for i in range(n_images):\n",
    "        l = len(descriptor_list[i])\n",
    "        for j in range(l):\n",
    "            if kmeans_ret is None:\n",
    "                idx = kmeans_ret[old_count+j]\n",
    "            else:\n",
    "                idx = kmeans_ret[old_count+j]\n",
    "            mega_histogram[i][idx] += 1\n",
    "        old_count += l\n",
    "    print (\"Vocabulary Histogram Generated\")\n",
    "    return mega_histogram\n",
    "    \n",
    "def train(train_labels, mega_histogram):\n",
    "    \"\"\"\n",
    "    uses sklearn.svm.SVC classifier (SVM) \n",
    "    \"\"\"\n",
    "    print (\"Training SVM\")\n",
    "    print (clf)\n",
    "    print (\"Train labels\", train_labels)\n",
    "    clf.fit(mega_histogram, train_labels)\n",
    "    print (\"Training completed\")  \n",
    "#after this is done send to training model pass the imlist\n",
    "#def recognize():\n",
    "    \n",
    "\n",
    "#def testModel(List):\n",
    "    \n",
    "\n",
    "def sift(image):\n",
    "    sift_object = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift_object.detectAndCompute(image, None)\n",
    "    return [keypoints, descriptors]\n",
    "\n",
    "def trainModel(imlist, count):\n",
    "        #no_clusters = no_clusters\n",
    "        train_path = None\n",
    "        test_path = None\n",
    "        #im_helper = ImageHelpers()\n",
    "        #bov_helper = BOVHelpers(no_clusters)\n",
    "        images = None\n",
    "        trainImageCount = 0\n",
    "        train_labels = np.array([])\n",
    "        name_dict = {}\n",
    "        descriptor_list = []\n",
    "        label_count = 0 \n",
    "\n",
    "        for word, imlist in imlist.items():\n",
    "            name_dict[str(label_count)] = word\n",
    "            print (\"Computing Features for \", word)\n",
    "            for im in imlist:\n",
    "                #cv2.imshow(word, im)\n",
    "                #cv2.waitKey()\n",
    "                train_labels = np.append(train_labels, label_count)\n",
    "                kp, des = sift(im)#perform sift here\n",
    "                descriptor_list.append(des)\n",
    "            print(\"sifting done\")\n",
    "            label_count += 1\n",
    "\n",
    "\n",
    "        # perform clustering\n",
    "        bov_descriptor_stack = formatND(descriptor_list)\n",
    "        cluster(bov_descriptor_stack)\n",
    "        hist = developVocabulary(count, descriptor_list = descriptor_list)\n",
    "\n",
    "        # show vocabulary trained\n",
    "        # self.bov_helper.plotHist()\n",
    " \n",
    "\n",
    "        stdhist = standardize(hist)\n",
    "        train(train_labels, stdhist)    \n",
    "    \n",
    "    \n",
    "path = \"Images/train/\"\n",
    "imlist = {}\n",
    "count = 0\n",
    "for each in glob(path + \"*\"):\n",
    "    word = each.split(\"/\")[-1]\n",
    "    #print (word)\n",
    "    imlist[word] = []\n",
    "    imgDir =  \"Images/\" + word \n",
    "    #print (imgDir)\n",
    "    data_path = os.path.join(imgDir,'*.jpg') \n",
    "    #print(data_path)\n",
    "    files = glob(data_path)\n",
    "    for imagefile in files:\n",
    "        #print (\"Reading file \", imagefile)\n",
    "        im = cv2.imread(imagefile, 0)\n",
    "        imlist[word].append(im)\n",
    "        count +=1\n",
    "        \n",
    "print(\"images loaded in\")\n",
    "trainModel(imlist, count)\n",
    "#print(imlist[word])\n",
    "\n",
    "\n",
    "#in training model perform sift & K clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
